{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-22T09:27:13.303611Z","iopub.execute_input":"2022-01-22T09:27:13.303931Z","iopub.status.idle":"2022-01-22T09:27:18.497065Z","shell.execute_reply.started":"2022-01-22T09:27:13.303898Z","shell.execute_reply":"2022-01-22T09:27:18.459397Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from glob import glob\ntrain_img_paths = sorted(glob('../input/carvana-image-masking-challenge-binary-png/train_images/*.jpg'))[:1000]\ntrain_mask_paths = sorted(glob('../input/carvana-image-masking-challenge-binary-png/train_masks/*.png'))[:1000]","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:27:38.773292Z","iopub.execute_input":"2022-01-22T09:27:38.773635Z","iopub.status.idle":"2022-01-22T09:27:38.823067Z","shell.execute_reply.started":"2022-01-22T09:27:38.773599Z","shell.execute_reply":"2022-01-22T09:27:38.822177Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"IMG_ROWS = 480\nIMG_COLS = 320\n\nTEST_IMG_ROWS = 1918\nTEST_IMG_COLS = 1280\n\nimport cv2\nimport imageio\ntrain_imgs = np.array([cv2.resize(imageio.imread(path), (IMG_ROWS, IMG_COLS))\n                        for path in train_img_paths])\n\ntrain_masks = np.array([cv2.resize(imageio.imread(path), (IMG_ROWS, IMG_COLS))\n                        for path in train_mask_paths])\n\ntrain_masks = train_masks.astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:27:44.175520Z","iopub.execute_input":"2022-01-22T09:27:44.175797Z","iopub.status.idle":"2022-01-22T09:28:59.240113Z","shell.execute_reply.started":"2022-01-22T09:27:44.175764Z","shell.execute_reply":"2022-01-22T09:28:59.239073Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nfig = plt.figure(0, figsize=(20, 20))\nfig.add_subplot(1, 2, 1)\nplt.imshow(train_imgs[19])\nfig.add_subplot(1, 2, 2)\nplt.imshow(np.squeeze(train_masks[19]), cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:29:00.498589Z","iopub.execute_input":"2022-01-22T09:29:00.498864Z","iopub.status.idle":"2022-01-22T09:29:01.009243Z","shell.execute_reply.started":"2022-01-22T09:29:00.498828Z","shell.execute_reply":"2022-01-22T09:29:01.008132Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import BatchNormalization\nfrom keras.layers import concatenate\nfrom keras.layers import Lambda\nfrom keras.models import Model","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:29:01.010902Z","iopub.execute_input":"2022-01-22T09:29:01.011372Z","iopub.status.idle":"2022-01-22T09:29:06.872263Z","shell.execute_reply.started":"2022-01-22T09:29:01.011319Z","shell.execute_reply":"2022-01-22T09:29:06.871404Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"inputs = Input((IMG_COLS, IMG_ROWS, 3))\n\nbnorm1 = BatchNormalization()(inputs)\nconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(bnorm1)\nconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\npool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\nconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\nconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\npool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\nconv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\nconv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\npool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\nconv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\nconv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\npool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\nconv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\nconv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n\nup6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\nconv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\nconv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n\nup7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\nconv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\nconv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n\nup8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\nconv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\nconv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n\nup9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\nconv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\nconv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n\nconv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n\nmodel = Model(inputs=[inputs], outputs=[conv10])","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:29:06.873687Z","iopub.execute_input":"2022-01-22T09:29:06.874795Z","iopub.status.idle":"2022-01-22T09:29:07.297456Z","shell.execute_reply.started":"2022-01-22T09:29:06.874744Z","shell.execute_reply":"2022-01-22T09:29:07.296646Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\nfrom keras.losses import binary_crossentropy\n\nSMOOTH = 1.\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + SMOOTH) / (K.sum(y_true_f) + K.sum(y_pred_f) + SMOOTH)\n\ndef bce_dice_loss(y_true, y_pred):\n    return 0.5 * binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:29:07.298934Z","iopub.execute_input":"2022-01-22T09:29:07.299473Z","iopub.status.idle":"2022-01-22T09:29:07.308331Z","shell.execute_reply.started":"2022-01-22T09:29:07.299425Z","shell.execute_reply":"2022-01-22T09:29:07.307389Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nmodel.compile(Adam(learning_rate=1e-4),\n              bce_dice_loss,\n              metrics=[binary_crossentropy, dice_coef])","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:29:07.309546Z","iopub.execute_input":"2022-01-22T09:29:07.309776Z","iopub.status.idle":"2022-01-22T09:29:07.725432Z","shell.execute_reply.started":"2022-01-22T09:29:07.309746Z","shell.execute_reply":"2022-01-22T09:29:07.724522Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model.fit(train_imgs[50:], train_masks[50:],\n          batch_size=12, epochs=10, \n          validation_data=(train_imgs[:50], train_masks[:50]))","metadata":{"execution":{"iopub.status.busy":"2022-01-22T09:29:07.727188Z","iopub.execute_input":"2022-01-22T09:29:07.727544Z","iopub.status.idle":"2022-01-22T15:46:13.323554Z","shell.execute_reply.started":"2022-01-22T09:29:07.727497Z","shell.execute_reply":"2022-01-22T15:46:13.322493Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model.save('/kaggle/output/carvana.h5')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T15:46:13.327729Z","iopub.execute_input":"2022-01-22T15:46:13.327974Z","iopub.status.idle":"2022-01-22T15:46:13.829711Z","shell.execute_reply.started":"2022-01-22T15:46:13.327943Z","shell.execute_reply":"2022-01-22T15:46:13.828670Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def test_img_generator(test_paths):\n    while True:\n        for path in test_paths:\n            yield np.array([cv2.resize(imageio.imread(path), (IMG_ROWS, IMG_COLS))])","metadata":{"execution":{"iopub.status.busy":"2022-01-22T16:04:48.714630Z","iopub.execute_input":"2022-01-22T16:04:48.714999Z","iopub.status.idle":"2022-01-22T16:04:48.723176Z","shell.execute_reply.started":"2022-01-22T16:04:48.714963Z","shell.execute_reply":"2022-01-22T16:04:48.722290Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(test_img_generator(test_paths[:10]), len(test_paths[:10]))","metadata":{"execution":{"iopub.status.busy":"2022-01-22T16:05:20.771452Z","iopub.execute_input":"2022-01-22T16:05:20.771839Z","iopub.status.idle":"2022-01-22T16:05:21.263609Z","shell.execute_reply.started":"2022-01-22T16:05:20.771802Z","shell.execute_reply":"2022-01-22T16:05:21.262208Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Проверка обученной модели невозможна, из-за отсутствия возможности верифицировать свой телефонный номер.","metadata":{}}]}
