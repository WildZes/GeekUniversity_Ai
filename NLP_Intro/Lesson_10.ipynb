{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson_10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Введение в обработку естественного языка\n",
        "## Урок 10. Машинный перевод. Модель seq2seq и механизм внимания\n",
        "### Разобраться с моделькой перевода как она устроена\n",
        "запустить для перевода с русского на английский (при желании можно взять другие пары языков) два варианта с вниманием и без внимания\n",
        "оценить качество насколько корректно переводит (для теста отобрать примеры с увеличением длины текста) (так как оценка визуальная достаточно 20-ти примеров в тестовой выборке)"
      ],
      "metadata": {
        "id": "g-pccqW5HVEX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuajw6yqHTzc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.manythings.org/anki/bel-eng.zip\n",
        "!unzip bel-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBkcP4yDHxBz",
        "outputId": "abe5d1c9-9f3e-4d82-d216-3e7b936b2c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-06 13:28:20--  http://www.manythings.org/anki/bel-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 150772 (147K) [application/zip]\n",
            "Saving to: ‘bel-eng.zip.2’\n",
            "\n",
            "bel-eng.zip.2       100%[===================>] 147.24K   314KB/s    in 0.5s    \n",
            "\n",
            "2022-07-06 13:28:21 (314 KB/s) - ‘bel-eng.zip.2’ saved [150772/150772]\n",
            "\n",
            "Archive:  bel-eng.zip\n",
            "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = w.lower().strip()\n",
        "\n",
        "  w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "metadata": {
        "id": "D5ZSEs4sINGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "metadata": {
        "id": "NsDMcmdZIkfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "metadata": {
        "id": "84JyABTPJBDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "metadata": {
        "id": "El83FaO_JCts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en, bel = create_dataset('/content/bel.txt', None)"
      ],
      "metadata": {
        "id": "65JwZTwRIuH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = 100000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset('/content/bel.txt', num_examples)\n",
        "\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "metadata": {
        "id": "Cx_buTJTI8TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 300\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))"
      ],
      "metadata": {
        "id": "22CFc8rCJb2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=False,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "metadata": {
        "id": "F-pI8blossXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_hidden = encoder(example_input_batch, sample_hidden)"
      ],
      "metadata": {
        "id": "h5pl63Alsv8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state"
      ],
      "metadata": {
        "id": "Q-AHST7os_PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden)"
      ],
      "metadata": {
        "id": "b1inZTGotCe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "xj8iINSntNaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_prefix = os.path.join('/content/', \"ckpt\")\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "metadata": {
        "id": "JdhfOUMbtS-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_hidden = encoder(inp, enc_hidden)\n",
        "        \n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "metadata": {
        "id": "BGCRXEDItjaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "  print('Time per epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvCPxkNjty40",
        "outputId": "f56bbfa0-afd2-49ba-cbc3-dbec994b67f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 1.9375\n",
            "Epoch 1 Loss 1.3123\n",
            "Time per epoch 218.06753039360046 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.2012\n",
            "Epoch 2 Loss 1.0963\n",
            "Time per epoch 173.29698133468628 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.9449\n",
            "Epoch 3 Loss 0.9666\n",
            "Time per epoch 173.22683382034302 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.9658\n",
            "Epoch 4 Loss 0.8608\n",
            "Time per epoch 173.3407278060913 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.8331\n",
            "Epoch 5 Loss 0.7689\n",
            "Time per epoch 174.29558157920837 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.5898\n",
            "Epoch 6 Loss 0.6805\n",
            "Time per epoch 175.1063437461853 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.6655\n",
            "Epoch 7 Loss 0.5979\n",
            "Time per epoch 180.23202109336853 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.4867\n",
            "Epoch 8 Loss 0.5138\n",
            "Time per epoch 175.5172402858734 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.3999\n",
            "Epoch 9 Loss 0.4408\n",
            "Time per epoch 174.1516215801239 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.3676\n",
            "Epoch 10 Loss 0.3745\n",
            "Time per epoch 173.3471872806549 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.3089\n",
            "Epoch 11 Loss 0.3221\n",
            "Time per epoch 176.7616684436798 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.2489\n",
            "Epoch 12 Loss 0.2791\n",
            "Time per epoch 176.6822693347931 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.2224\n",
            "Epoch 13 Loss 0.2418\n",
            "Time per epoch 181.4834508895874 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.2054\n",
            "Epoch 14 Loss 0.2119\n",
            "Time per epoch 178.6064476966858 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.1787\n",
            "Epoch 15 Loss 0.1835\n",
            "Time per epoch 175.40107917785645 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1360\n",
            "Epoch 16 Loss 0.1593\n",
            "Time per epoch 178.15628242492676 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1198\n",
            "Epoch 17 Loss 0.1408\n",
            "Time per epoch 175.486807346344 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.1096\n",
            "Epoch 18 Loss 0.1252\n",
            "Time per epoch 179.27651071548462 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0851\n",
            "Epoch 19 Loss 0.1099\n",
            "Time per epoch 175.88201355934143 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0883\n",
            "Epoch 20 Loss 0.0949\n",
            "Time per epoch 175.7412610054016 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0702\n",
            "Epoch 21 Loss 0.0823\n",
            "Time per epoch 174.50931668281555 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0682\n",
            "Epoch 22 Loss 0.0730\n",
            "Time per epoch 177.62647151947021 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0544\n",
            "Epoch 23 Loss 0.0630\n",
            "Time per epoch 178.15077662467957 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0475\n",
            "Epoch 24 Loss 0.0553\n",
            "Time per epoch 174.66080713272095 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0333\n",
            "Epoch 25 Loss 0.0480\n",
            "Time per epoch 179.84692096710205 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0342\n",
            "Epoch 26 Loss 0.0396\n",
            "Time per epoch 177.03766202926636 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0254\n",
            "Epoch 27 Loss 0.0346\n",
            "Time per epoch 176.9026596546173 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0225\n",
            "Epoch 28 Loss 0.0307\n",
            "Time per epoch 180.58140444755554 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0204\n",
            "Epoch 29 Loss 0.0268\n",
            "Time per epoch 177.8391399383545 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0258\n",
            "Epoch 30 Loss 0.0238\n",
            "Time per epoch 176.57482266426086 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0150\n",
            "Epoch 31 Loss 0.0215\n",
            "Time per epoch 177.5777645111084 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0198\n",
            "Epoch 32 Loss 0.0194\n",
            "Time per epoch 180.71367526054382 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0152\n",
            "Epoch 33 Loss 0.0174\n",
            "Time per epoch 178.4973247051239 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0057\n",
            "Epoch 34 Loss 0.0160\n",
            "Time per epoch 180.3494369983673 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0068\n",
            "Epoch 35 Loss 0.0147\n",
            "Time per epoch 178.8993263244629 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0057\n",
            "Epoch 36 Loss 0.0140\n",
            "Time per epoch 180.3201584815979 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0094\n",
            "Epoch 37 Loss 0.0129\n",
            "Time per epoch 181.52706789970398 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0088\n",
            "Epoch 38 Loss 0.0124\n",
            "Time per epoch 179.5091552734375 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0100\n",
            "Epoch 39 Loss 0.0123\n",
            "Time per epoch 180.28425550460815 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0087\n",
            "Epoch 40 Loss 0.0123\n",
            "Time per epoch 180.72563910484314 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0087\n",
            "Epoch 41 Loss 0.0121\n",
            "Time per epoch 180.2106535434723 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0146\n",
            "Epoch 42 Loss 0.0122\n",
            "Time per epoch 178.97696256637573 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0090\n",
            "Epoch 43 Loss 0.0121\n",
            "Time per epoch 179.7854447364807 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0097\n",
            "Epoch 44 Loss 0.0130\n",
            "Time per epoch 182.06438517570496 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0138\n",
            "Epoch 45 Loss 0.0135\n",
            "Time per epoch 180.9718782901764 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0056\n",
            "Epoch 46 Loss 0.0142\n",
            "Time per epoch 177.9110209941864 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0124\n",
            "Epoch 47 Loss 0.0185\n",
            "Time per epoch 176.65051221847534 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0097\n",
            "Epoch 48 Loss 0.0235\n",
            "Time per epoch 178.33485341072083 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0233\n",
            "Epoch 49 Loss 0.0246\n",
            "Time per epoch 178.12902903556824 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0201\n",
            "Epoch 50 Loss 0.0297\n",
            "Time per epoch 181.66850447654724 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    \n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    \n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "   \n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    \n",
        "    enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "metadata": {
        "id": "3iiRmEheuI-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "  result, sentence = evaluate(sentence)"
      ],
      "metadata": {
        "id": "JW8VdGp9RJqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('Ружовыя акуляры.')"
      ],
      "metadata": {
        "id": "cdqAvNB4RRTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('ды хадзі шчаслівы.')"
      ],
      "metadata": {
        "id": "fv3ySwz-Stl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('Навошта табе ды?')"
      ],
      "metadata": {
        "id": "7Js5QvWbTBd8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}